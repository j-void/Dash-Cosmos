{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9fd6de",
   "metadata": {},
   "source": [
    "### COSMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da983437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from utils.data_loader import CosmosTestDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score\n",
    "import utils.misc as misc\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, bboxes, caption_match, caption_diff, label  = zip(*batch)\n",
    "    return list(images), list(bboxes), list(caption_match), list(caption_diff), list(label)\n",
    "\n",
    "transform_full = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])    \n",
    "\n",
    "test_dataset = CosmosTestDataset(json_file=\"data/cosmos_anns_acm/cosmos_anns_acm/acm_anns/public_test_acm.json\", \\\n",
    "    img_dir=\"data\", transform_full=transform_full)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=1\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3190c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def visualize_results(preds, gts, dataloader, k=5):\n",
    "    indices = random.sample(range(len(preds)), k)\n",
    "    \n",
    "    for idx in indices:\n",
    "        # Get data\n",
    "        image, bboxes, caption1, caption2, label, bert_score = dataloader[idx]\n",
    "        pred_label = preds[idx]\n",
    "        gt_label = gts[idx]\n",
    "\n",
    "        # Plot image\n",
    "        plt.imshow(image.permute(1, 2, 0))  # if image is a tensor with shape (C, H, W)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Prediction: {'Out-of-context' if pred_label else 'In-context'} | \"\n",
    "                  f\"Ground Truth: {'Out-of-context' if gt_label else 'In-context'} | \"\n",
    "                  f\"{'✓' if pred_label == gt_label else '✗'}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Print captions\n",
    "        print(f\"Caption 1: {caption1}\")\n",
    "        print(f\"Caption 2: {caption2}\")\n",
    "        print(f\"BERT Score: {bert_score:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.cosmos import CosmosFullModel\n",
    "\n",
    "\n",
    "class CosmosTest:\n",
    "    def __init__(self, load_path, dataloader, device):\n",
    "        self.device = device\n",
    "        self.dataloader = dataloader\n",
    "        self.model = CosmosFullModel(300, \"cuda\")\n",
    "        checkpoint = torch.load(load_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "    def get_prediction(self, caption1_scores, caption2_scores, bboxes, bert_score):\n",
    "        caption1_bboxes = misc.top_bbox_from_scores(bboxes, caption1_scores)\n",
    "        caption2_bboxes = misc.top_bbox_from_scores(bboxes, caption2_scores)\n",
    "        bbox_overlap = misc.is_bbox_overlap(caption1_bboxes, caption2_bboxes, 0.5)\n",
    "        if bbox_overlap:\n",
    "            if bert_score >= 0.5:\n",
    "                context = 0\n",
    "            else:\n",
    "                context = 1\n",
    "            return context\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def run_test(self):\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for image, bboxes, caption1, caption2, label, bert_score in tqdm(dataloader, desc=\"Test Batch:\"):\n",
    "                object_embeddings, match_embeddings, diff_embeddings = model(image, bboxes, caption1, caption2)\n",
    "                caption1_scores, caption2_scores = misc.get_scores(object_embeddings, match_embeddings, diff_embeddings)\n",
    "                \n",
    "                preds = self.get_prediction(caption1_scores[0], caption2_scores[0], bboxes[0], bert_score[0])\n",
    "                labels = label.item()  # All match cases are positives\n",
    "                \n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels)\n",
    "                \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        ap = average_precision_score(all_labels, all_preds)       \n",
    "        \n",
    "        tqdm.write(f\"Test Metrics: Accuracy = {accuracy}, F1 Score = {f1}, Average Precision = {ap}\") \n",
    "        \n",
    "        return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26efd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108eb9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808dd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
